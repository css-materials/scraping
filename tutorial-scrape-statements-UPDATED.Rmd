---
title: "Scraping Presidential Statements"
author: Sabrina Nardin
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,    
  eval = TRUE,
  message = FALSE,
  warning = FALSE
)
```

### Scaling Up Our Scraper

So far, we applied a function (see slides) to scrape information from one or two pages: we randomly picked a few presidential **messages**. We learned how to scrape by inspecting the HTML and by using the SelectorGadget. The URL we used so far is under the messages category (with over 12,000 messages), for example: `https://www.presidency.ucsb.edu/documents/app-categories/citations/presidential/messages?field_docs_start_date_time_value%5Bvalue%5D%5Bdate%5D=1958&page=4`

In this tutorial, we switch from messages to **letters**, but the logic is exactly the same. We have over 4,000  presidential letters on that online archive. 

**If we want to scrape all pages (all presidential letters), what steps should we take to do this efficiently?** 

Note if you go to `https://www.presidency.ucsb.edu/documents` you see all "Categories" of documents you could scrape (executive orders, fireside chats, letters, messages, statements, etc.).  

For example, there are over 4,000 presidential letters, each with its own unique URL. How can we further automate our scraper so that we do not have to manually pass 4000+ URLs into our previous function each time? 

To tackle this challenge, we first need to plan: explore the website to identify the best page to use as a starting point and examine how that page is built. 

### Choosing a Starting Page and Planning our Code

Since we want to collect all letters, the most suitable starting point is the page that provides links to all 4,000+ letters, ordered from the most recent to the earliest (1797):

`https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters`

Open that page and explore it. You should notice it displays only 10 URLs at a time, but if you turn pages you get an additional 10 URLs per page, until you access all URLs (one for each letter).

So we want our code to:

* Start from the initial page, collect all letter URLs from it, and store them into a character vector (or a list).
* Turn pages and collect all URLs from each subsequent page, continuing this process until the last page. Remember there are 4,000+ letters, spread across 400+ pages.
* Finally, apply our `scrape_doc` function (see last lecture slides) to each of the 4,000+ URLs we collected, one by one, to get all info from each letter (name, title, date, full text).


### Collecting All URLs From the First Page

Load required libraries:
```{r}
library(rvest)
library(tidyverse)
library(lubridate)
```

Initial URL for the letters listing (page 1):
```{r}
letters_page1_url <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
```

First, read the HTML content into R:
```{r}
letters_page1_html <- read_html(letters_page1_url)
letters_page1_html
# letters_page1_html |> as.character() |> cat()
```

Our goal is to collect all (10) URLs from this initial page:
- Inspect the HTML of the page and check a few URLs before implementing the code.
- We can note that the links we need are, as expected, under a `a` with `href` attribute, which is nested under a `p` tag, and a `div` tag with `class="field-title"` attribute
- We try a few different options using and then use the selector that is most useful here.  Note: you cannot pass the raw HTML string (`div class="field-title"`) directly; you need to write the selector in the way `rvest` expects (e.g., `"div.field-title"`).

```{r}
# option 1 = not specific enough
# html_elements(letters_page1_html, "div")

# option 2 (more precise) = works
# html_elements(letters_page1_html, "div.field-title")

# option 3 (even more precise, full path) = works
html_elements(letters_page1_html, "div.field-title p a")
```

Now we can extract the actual links. From last lecture, we know we can use `html_attr()` to retrieve HTML attribute values: here the tag is `<a>`, the attribute is `href`, and the actual link is the value.

```{r}
# get actual link as string
letter_links_rel_page1 <- letters_page1_html %>%
  html_elements("div.field-title p") %>%
  html_attr("href")

# check
length(letter_links_rel_page1)
letter_links_rel_page1[1:3]
is.character(letter_links_rel_page1)
```

### Relative vs Full URLs

The links we got with this code are "relative" URLs, meaning they are incomplete: they lack the "base" part of the URL. If you try to use them directly in your browser, they won't work because the browser won't know the full address to navigate to.

So let's add the "base" part of the URL. You can use the `paste()` function from base R or `paste0()`. They both concatenates character vectors:

```{r}
# base domain for all pages on this site
domain_url <- "https://www.presidency.ucsb.edu"

# build full path
letter_links_full_page1 <- paste(domain_url, letter_links_rel_page1, sep = "")

# check
length(letter_links_full_page1)
letter_links_full_page1[1:3]
```

Now we have all links from the first page (`https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters`), but we need the letter links from the other pages. From the website, click on the numbers that allow to turn page and observe how the URL changes.

From the website, click on the numbers that allow you to turn pages and observe how the URL changes. For example:

* Page 1 (already collected):  
  `https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters`
* Page 2:  
  `https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters?page=1`
* Page 3:  
  `https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters?page=2`
* Last page (as of now, check the website for updates!):  
  `https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters?page=478` 

We can see:

- The part before `?page=` stays the same: this is the "base" URL
- The part after `?page=` changes from 1 up to 478

We can generate these URLs automatically:

```{r}
# listing base URL (letters category)
listing_base_url <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"

# number of pages to turn (as of now; this may change over time)
num_pages <- 478

# initialize empty vector to store all listing page URLs (page 2 to last)
listing_page_urls <- vector("character", num_pages)

for (i in 1:num_pages) {
  listing_page_urls[i] <- paste0(listing_base_url, "?page=", i)
}

# check
listing_page_urls[1:3]
length(listing_page_urls)
```

Now add at position 1 the initial page, which  we already looked at:
```{r}
# add 
page_urls <- c(listing_base_url, listing_page_urls)

# check
page_urls[1:3]
length(page_urls)
```

So `page_urls` is a character vector containing all listing pages for the presidential letters (page 1 through page 477 in total).


### Scraping Letter Links From All Listing Pages

Recall this is the code we have written so far to scrape the links to the letters from the first page:
```{r}
letters_page1_html <- read_html(letters_page1_url)

letter_links_rel_page1 <- letters_page1_html %>%
  html_elements("div.field-title p a") %>%
  html_attr("href")

letter_links_full_page1 <- paste0(domain_url, letter_links_rel_page1)
letter_links_full_page1[1:3]
```

Now we put this logic into a function so we can apply it to each listing page to scrape letter links:
```{r}
links_all_pages <- function(page_url) {
  # be polite: pause 1–3 seconds between requests
  Sys.sleep(runif(1, min = 1, max = 3))

  # get HTML for the listing page
  page_html <- read_html(page_url)

  # extract relative links for letters
  relative_links <- page_html %>% 
    html_elements("div.field-title p a") %>%
    html_attr("href")

  # convert to full URLs
  full_links <- paste0(domain_url, relative_links)

  return(full_links)
}
```

Test this function out with the first couple of pages stored in `page_urls`.  What should we get back? How many results per page?
```{r}
# test on first listing page (page 1)
test_links_1 <- links_all_pages(page_urls[1])
length(test_links_1)
head(test_links_1, 3)

# test on second listing page (page 2)
test_links_2 <- links_all_pages(page_urls[2])
length(test_links_2)
head(test_links_2, 3)

# etc. try to test this on other pages
```

### Putting It All Togheter

Now, we can run the function to collect the links from all pages, BUT FOR CLASS WE DO NOT RUN THIS FOR ALL 400+ PAGES: it would take a long time and could put unnecessary load on the website’s server. Instead, we will limit ourselves to the first 2–3 pages and build from there, like that:
```{r}
# select only a few listing pages (for demo)
selected_pages <- page_urls[1:2]
selected_pages
```

We will store the links from each selected listing page into a vector. Note: here we are using a vector, but you could also use a list. Lists can be bit more challenging to use than vectors but more flexible. Try rewriting the code below using a list to store the results rather than a vector, and observe the differences:
```{r}
# preallocate a vector slightly larger than needed
# each page has ~10 links, we have selected 3 pages, thus 30 links
all_urls  <- vector("character", length(selected_pages) * 10)
#length(all_urls)

# counter
url_index <- 1

# loop over selected pages and store the links

# outer loop: iterates through selected pages
for (i in seq_along(selected_pages)) {
  page_links <- links_all_pages(selected_pages[i])
  
  # inner loop: loop through each URL
  for (link in page_links) {
    all_urls[url_index] <- link
    url_index <- url_index + 1  # move to the next index
  }
}

length(all_urls)
head(all_urls)
```

At this point: `all_urls` contains the full URLs for the letters from the selected listing pages. To scale up, you would apply the same logic to all `page_urls` (but that will take a while to scrape!), not just `selected_pages`.


### Next Step: Scraping the Actual Document Data!

1. Now we have all the letter URLs (or at least a subset for class). The next step is to scrape the actual content (name, date, title, full text) from each letter URL. To do that take the function `scrape_doc` from last lecture, and apply it, or adjust it if needed:
```{r}
scrape_doc <- function(url) {
  # Scrapes data from presidential pages pausing between requests
  # Args:
    # url (string): one presidential page 
  # Returns:
    # tibble: a tibble with the date, speaker, title, full text from input url

  # get HTML page
  Sys.sleep(2)
  url_contents <- read_html(x = url)

  # extract elements
  date <- html_elements(x = url_contents, css = ".date-display-single") %>%
    html_text2() %>% mdy()

  name <- html_elements(x = url_contents, css = ".diet-title a") %>%
    html_text2()
  
  title <- html_elements(x = url_contents, css = "h1") %>%
    html_text2()
  
  text <- html_elements(x = url_contents, css = "div.field-docs-content") %>%
    html_text2()
  
  # store in a data frame and return it
  url_data <- tibble(
    date = date,
    name = name,
    title = title,
    text = text
  )
  
  return(url_data)
}
```

```{r}
# example of applying scrape_doc() to many URLs
# note some data cleaning is needed, but it works!

results_list <- vector("list", length(all_urls))
 
for (i in seq_along(all_urls)) {
   results_list[[i]] <- scrape_doc(all_urls[i])
 }
 
letters_df <- bind_rows(results_list)
letters_df
```

2. Add error-handling statements to your code to manage potential issues you might encounter while scraping. This is because our code will be be sending over 4000+ requests for data (one per letter page), and some of these requests might fail. The most common issue is a page denying your data request, resulting in a "404 error." To tackle this challenge, you need to write code that uses conditional statements to send a request to the webpage you want to scrape; if denied, the code should raise an error message.
```{r}

```
