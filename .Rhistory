library(rvest)
library(tidyverse)
page_one <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
page_one <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
# Read the HTML content into R
page <- read_html(page_one)
page
page |> as.character() |> cat()
#page |> as.character() |> cat()
```
#page |> as.character() |> cat()
```
# Read the HTML content into R
page <- read_html(page_one)
page
#page |> as.character() |> cat()
# option 1 = not specific enough
html_elements(page, "div")
# option 2 (more precise) = works
html_elements(page, "div.field-title")
# option 3 (same as option 2 but shorter) = works
html_elements(page, ".field-title")
# option 4 (even more precise, full path) = works
# try first with p then with p a
html_elements(page, "div.field-title p a")
# notice the difference btw html_element() and html_elements()
links_page_one <- page %>%
html_elements("div.field-title p a") %>%
html_attr("href")
links_page_one
is.vector(links_page_one)
is.character(links_page_one)
base_url <- "https://www.presidency.ucsb.edu"
# paste
paste(base_url, links_page_one)
fl <- paste(base_url, links_page_one, sep = "")
# paste0
full_links <- paste0(base_url, links_page_one)
# check
identical(fl, full_links)
# determine base url
base_url <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
# determine how many pages we need to turn
num_pages <- 476
# initialize empty vector to store all urls
page_urls <- vector("character", num_pages)
page_urls
length(page_urls)
# loop to populate with data this empty vector
for (i in 1:num_pages) {
page_urls[i] <- paste0(base_url, "?page=", i)
}
page_urls
# add at position 1 the page we already collected data from
# it has a slightly different structure
# replace this page_urls[1] <- base_url
#with
page_urls <- c(base_url, page_urls)
page_urls[1:5]
# note this is a simple character vector
is.character(page_urls)
page_one <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
page <- read_html(page_one)
links_page_one <- page %>%
html_elements("div.field-title p a") %>%
html_attr("href")
links_page_one
# best
page <- read_html(p)
page1 <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
page1_read <- read_html(page1)
links_page1 <- page1_read %>%
html_elements("div.field-title p a") %>%
html_attr("href")
links_page1
# best is leaving the read_html outside the function
links_all_pages <- function(page) {
page %>%
html_elements("div.field-title p a") %>%
html_attr("href")
}
page_urls[1]
page1
# note this is a simple character vector
#is.character(page_urls)
```
# determine base url
base_url <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
# determine how many pages we need to turn
num_pages <- 476
# initialize empty vector to store all urls
page_urls <- vector("character", num_pages)
page_urls
length(page_urls)
# loop to populate with data this empty vector
for (i in 1:num_pages) {
page_urls[i] <- paste0(base_url, "?page=", i)
}
page_urls
# add at position 1 the page we already collected data from
# it has a slightly different structure
# replace this page_urls[1] <- base_url
#with
#page_urls <- c(base_url, page_urls)
page_urls[1:5]
# note this is a simple character vector
#is.character(page_urls)
for (i in seq_along(page_urls)) {
print(i)
print(page_urls[i])
}
library(rvest)
library(tidyverse)
page_one <- "https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters"
html_elements(page, "div")
html_elements(page, "div.field-title")
html_elements(page, "div.field-title")
html_elements(page, ".field-title")
html_elements(page, "div.field-title p a")
links_page_one <- page %>%
html_elements("div.field-title p a") %>%
html_attr("href")
#links_page_one
is.vector(links_page_one)
is.character(links_page_one)
links_page_one <- page %>%
html_elements("div.field-title p a") %>%
html_attr("href")
links_page_one[1:3]
length(links_page_one)
#is.vector(links_page_one)
#is.character(links_page_one)
page_urls <- vector("character", num_pages)
page_urls
length(page_urls)
for (i in 1:num_pages) {
page_urls[i] <- paste0(base_url, "?page=", i)
}
page_urls
fl
for (i in 1:num_pages) {
page_urls[i] <- paste0(base_url, "?page=", i)
}
page_urls
page_urls[1:3]
page_urls <- c(base_url, page_urls)
page_urls[1:5]
#page_urls
page_urls[1:3]
page_urls[1:3]
links_all_pages <- function(p) {
Sys.sleep(runif(1, min = 1, max = 3))
#Sys.sleep(1)
page <- read_html(p)
links <- page %>%
html_elements("div.field-title p a") %>%
html_attr("href")
return(links)
}
links_all_pages(page_urls[1])
#links_all_pages("https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters")
links_all_pages(page_urls[2])
#links_all_pages("https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters?page=2")
links_all_pages(page_urls[3])
#links_all_pages("https://www.presidency.ucsb.edu/documents/app-categories/presidential/letters?page=2")
for (i in seq_along(page_urls)) {
print(i)
print(page_urls[i])
}
# now run function to collect the links, BUT limit !
for (i in seq_along(page_urls[1:3])) {
print(links_all_pages(page_urls[i]))
}
# now store output
selection <- page_urls[1:2]
all_urls <- vector("character", length(selection) * 10)
length(all_urls)
# counter
url_index <- 1
# outer loop: iterates through the selected URLs only
for (i in seq_along(selection)) {
# get the links from the current page
page_links <- links_all_pages(selection[i])
print(page_links)
# inner loop: loop through each link on the page and stores them
for (link in page_links) {
# append each link to the all_urls vector
all_urls[url_index] <- link
url_index <- url_index + 1  # move to the next index
}
}
all_urls
all_urls
all_urls
